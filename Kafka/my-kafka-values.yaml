
## Kafka Chart Configuration

##  CONFIGURE MORE BROKERS:
  # When service type is NodePort and you want more than 1 broker you have to You will have a different node port for each Kafka broker!
  # So, when you set replicaCount values, adjust also the externalAccess.service.nodePorts array.
  # In this way the setup.sh script in the configMap can set up properly each Broker to configure Kafka external listener.
  # You can get the list of configured node ports using the command below:
          # echo "$(kubectl get svc --namespace default -l "app.kubernetes.io/name=kafka,app.kubernetes.io/instance=my-kafka,app.kubernetes.io/component=kafka,pod" -o jsonpath='{.items[*].spec.ports[0].nodePort}' | tr ' ' '\n')"


## Bitnami Kafka image version
## ref: https://hub.docker.com/r/bitnami/kafka/tags/
##
image:
  registry: docker.io
  repository: bitnami/kafka
  tag: 2.7.0-debian-10-r35
  ## Specify a imagePullPolicy
  ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
  ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
  ##
  pullPolicy: IfNotPresent
  ## Optionally specify an array of imagePullSecrets (secrets must be manually created in the namespace)
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
  ## Example:
  ## pullSecrets:
  ##   - myRegistryKeySecretName
  ##
  pullSecrets: []


## Specify content for server.properties
## NOTE: This will OVERRIDE(!!) any KAFKA_CFG_ environment variables (including those set by the chart)
## The server.properties is auto-generated based on other parameters when this parameter is not specified
##
# config: |-
#   advertised.listeners=EXTERNAL://$(curl -s https://ipinfo.io/ip):30001

## Example:
# config: |-
#   broker.id=-1
#   listeners=PLAINTEXT://:9092
#   advertised.listeners=PLAINTEXT://KAFKA_IP:9092
#   num.network.threads=3
#   num.io.threads=8
#   socket.send.buffer.bytes=102400
#   socket.receive.buffer.bytes=102400
#   socket.request.max.bytes=104857600
#   log.dirs=/bitnami/kafka/data
#   num.partitions=1
#   num.recovery.threads.per.data.dir=1
#   offsets.topic.replication.factor=1
#   transaction.state.log.replication.factor=1
#   transaction.state.log.min.isr=1
#   log.flush.interval.messages=10000
#   log.flush.interval.ms=1000
#   log.retention.hours=168
#   log.retention.bytes=1073741824
#   log.segment.bytes=1073741824
#   log.retention.check.interval.ms=300000
#   zookeeper.connect=ZOOKEEPER_SERVICE_NAME
#   zookeeper.connection.timeout.ms=6000
#   group.initial.rebalance.delay.ms=0
##

## All the parameters from the configuration file can be overwritten by using environment variables with this format: KAFKA_CFG_{KEY}
## ref: https://github.com/bitnami/bitnami-docker-kafka#configuration
## Example:
# extraEnvVars:
#   - name: KAFKA_CFG_ZOOKEEPER_CONNECT
#     value: "my-kafka-zookeeper"
##

## Number of Kafka brokers to deploy
##
replicaCount: 1

## Minimal broker.id value
## Brokers increment their ID starting at this minimal value.
## E.g., with `minBrokerId=100` and 3 nodes, IDs will be 100, 101, 102 for brokers 0, 1, and 2, respectively.
##
minBrokerId: 0


## Pod affinity preset
## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
## Allowed values: soft, hard
##
podAffinityPreset: ""

## Pod anti-affinity preset
## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
## Allowed values: soft, hard
##
podAntiAffinityPreset: soft

## Node affinity preset
## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
## Allowed values: soft, hard
##
nodeAffinityPreset:
  ## Node affinity type
  ## Allowed values: soft, hard
  ##
  type: ""
  ## Node label key to match
  ## E.g.
  ## key: "kubernetes.io/e2e-az-name"
  ##
  key: ""
  ## Node label values to match
  ## E.g.
  ## values:
  ##   - e2e-az1
  ##   - e2e-az2
  ##
  values: []

## Affinity for pod assignment
## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
## Note: podAffinityPreset, podAntiAffinityPreset, and  nodeAffinityPreset will be ignored when it's set
##
affinity: {}

## Node labels for pod assignment
## Ref: https://kubernetes.io/docs/user-guide/node-selection/
##
nodeSelector: {}

## Tolerations for pod assignment
## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
##
tolerations: []

## Kafka containers' liveness and readiness probes. Evaluated as a template.
## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
##
livenessProbe:
  enabled: false
  initialDelaySeconds: 20
  timeoutSeconds: 5
  # failureThreshold: 3
  # periodSeconds: 10
  # successThreshold: 1
readinessProbe:
  enabled: false
  initialDelaySeconds: 20
  failureThreshold: 6
  timeoutSeconds: 5
  # periodSeconds: 10
  # successThreshold: 1


## Add sidecars container to the pod.
## Example:
## sidecars:
##   - name: your-image-name
##     image: your-image
##     imagePullPolicy: Always
##     ports:
##       - name: portname
##         containerPort: 1234
##
# sidecars:
#   - name: kakfacat-debugger
#     image: edenhill/kafkacat:1.6.0
#     imagePullPolicy: IfNotPresent

## SERVICE PARAMETERS
##
service:
  ## Service type
  ##
  type: ClusterIP
  ## Kafka port for client connections
  ##
  port: 9092
  ## Kafka port for inter-broker connections
  ##
  internalPort: 9093
  ## Kafka port for external connections
  ##
  externalPort: 9094
  ## Specify the nodePort value for the LoadBalancer and NodePort service types.
  ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport
  ##
  nodePorts:
    client: ""
    external: "30001"
  ## Set the LoadBalancer service type to internal only.
  ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer
  ##
  # loadBalancerIP:
  ## Load Balancer sources
  ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
  ## Example:
  ## loadBalancerSourceRanges:
  ## - 10.10.10.0/24
  ##


## EXTERNAL ACCESS TO KAFKA BROKERS CONFIGURATION
##
externalAccess:
  ## Enable Kubernetes external cluster access to Kafka brokers
  ##
  enabled: true
  ## External IPs auto-discovery configuration
  ## An init container is used to auto-detect LB IPs or node ports by querying the K8s API
  ## Note: RBAC might be required
  ##
  autoDiscovery:
    ## Enable external IP/ports auto-discovery
    ##
    enabled: false
    ## Bitnami Kubectl image
    ## ref: https://hub.docker.com/r/bitnami/kubectl/tags/
    ##
    image:
      registry: docker.io
      repository: bitnami/kubectl
      tag: 1.17.17-debian-10-r9
      ## Specify a imagePullPolicy
      ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
      ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
      ##
      pullPolicy: IfNotPresent
      ## Optionally specify an array of imagePullSecrets (secrets must be manually created in the namespace)
      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
      ## Example:
      ## pullSecrets:
      ##   - myRegistryKeySecretName
      ##
      pullSecrets: []
    ## Init Container resource requests and limits
    ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
    ##
    resources:
      # We usually recommend not to specify default resources and to leave this as a conscious
      # choice for the user. This also increases chances charts run on environments with little
      # resources, such as Minikube. If you do want to specify resources, uncomment the following
      # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
      limits: {}
      #   cpu: 100m
      #   memory: 128Mi
      requests: {}
      #   cpu: 100m
      #   memory: 128Mi

  ## Parameters to configure K8s service(s) used to externally access Kafka brokers
  ## A new service per broker will be created
  ##
  service:
    ## Service type. Allowed values: LoadBalancer or NodePort
    ##
    type: NodePort
    ## Port used when service type is LoadBalancer
    ##
    port: 9094
    ## Array of load balancer IPs for each Kafka broker. Length must be the same as replicaCount
    ## Example:
    ## loadBalancerIPs:
    ##   - X.X.X.X
    ##   - Y.Y.Y.Y
    ##
    loadBalancerIPs: []
    ## Load Balancer sources
    ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
    ## Example:
    ## loadBalancerSourceRanges:
    ## - 10.10.10.0/24
    ##
    loadBalancerSourceRanges: []
    ## Array of node ports used for each Kafka broker. Length must be the same as replicaCount
    ## Example:
    ## nodePorts:
    ##   - 30001
    ##   - 30002
    ##
    nodePorts:
         - 30001
    
    ## When service type is NodePort, you can specify THE DOMAIN USED FOR KAFKA ADVERTISED LISTENERS.
    ## If not specified, the container will try to get the kubernetes node external IP
    ##
    domain: 127.0.0.1
    ## Provide any additional annotations which may be required. Evaluated as a template
    ##
    annotations: {}

## The address(es) the socket server listens on.
## When it's set to an empty array, the listeners will be configured
## based on the authentication protocols (auth.clientProtocol and auth.interBrokerProtocol parameters)
##
listeners: []

## The address(es) (hostname:port) the brokers will advertise to producers and consumers.
## When it's set to an empty array, the advertised listeners will be configured
## based on the authentication protocols (auth.clientProtocol and auth.interBrokerProtocol parameters)
##
advertisedListeners: []

## PERSISTENCE PARAMETERS
##
persistence:
  enabled: true
  ## A manually managed Persistent Volume and Claim
  ## If defined, PVC must be created manually before volume will be bound
  ## The value is evaluated as a template
  ##
  # existingClaim:
  ## PV Storage Class
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is
  ## set, choosing the default provisioner.
  ##
  # storageClass: "-"
  ## PV Access Mode
  ##
  accessModes:
    - ReadWriteOnce
  ## PVC size
  ##
  size: 8Gi
  ## PVC annotations
  ##
  annotations: {}
  ## Mount point for persistence
  ##
  mountPath: /bitnami/kafka

## Log Persistence parameters
##
logPersistence:
  enabled: false
  ## A manually managed Persistent Volume and Claim
  ## If defined, PVC must be created manually before volume will be bound
  ## The value is evaluated as a template
  ##
  # existingClaim:
  ## PV Storage Class
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is
  ## set, choosing the default provisioner.
  # existingLogClaim:
  ## PV Storage Class
  ## It getted from persistence.storageClass
  ##
  ## PV Access Mode
  ##
  accessModes:
    - ReadWriteOnce
  ## PVC size
  ##
  size: 8Gi
  ## PVC annotations
  ##
  annotations: {}
  ## Mount path for persistent logs
  ##
  mountPath: /opt/bitnami/kafka/logs

## Init Container parameters
## Change the owner and group of the persistent volume(s) mountpoint(s) to 'runAsUser:fsGroup' on each component
## values from the securityContext section of the component
##
volumePermissions:
  enabled: false
  ## The security context for the volumePermissions init container
  ##
  securityContext:
    runAsUser: 0
  ## Bitnami Minideb image
  ## ref: https://hub.docker.com/r/bitnami/minideb/tags/
  ##
  image:
    registry: docker.io
    repository: bitnami/minideb
    tag: buster
    ## Specify a imagePullPolicy
    ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
    ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
    ##
    pullPolicy: Always
    ## Optionally specify an array of imagePullSecrets (secrets must be manually created in the namespace)
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ## Example:
    ## pullSecrets:
    ##   - myRegistryKeySecretName
    ##
    pullSecrets: []
  ## Init Container resource requests and limits
  ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources:
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    limits: {}
    #   cpu: 100m
    #   memory: 128Mi
    requests: {}
    #   cpu: 100m
    #   memory: 128Mi

## Kafka pods ServiceAccount
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
##
serviceAccount:
  ## Specifies whether a ServiceAccount should be created
  ##
  create: true
  ## The name of the ServiceAccount to use.
  ## If not set and create is true, a name is generated using the fluentd.fullname template
  ##
  # name:

## Role Based Access
## ref: https://kubernetes.io/docs/admin/authorization/rbac/
##
rbac:
  ## Specifies whether RBAC rules should be created
  ## binding Kafka ServiceAccount to a role
  ## that allows Kafka pods querying the K8s API
  ##
  create: true

## Kafka provisioning - USEFUL TO CREATE TOPICS AUTOMATICALLY
##
provisioning:
  enabled: true

  image:
    registry: docker.io
    repository: bitnami/kafka
    tag: 2.7.0-debian-10-r34
    ## Specify a imagePullPolicy
    ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
    ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
    ##
    pullPolicy: IfNotPresent
    ## Optionally specify an array of imagePullSecrets (secrets must be manually created in the namespace)
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ## Example:
    ## pullSecrets:
    ##   - myRegistryKeySecretName
    ##
    pullSecrets: []

    ## Set to true if you would like to see extra information on logs
    ##
    debug: false

  resources:
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    limits: {}
    #   cpu: 250m
    #   memory: 1Gi
    requests: {}
    #   cpu: 250m
    #   memory: 256Mi

  ##TOPIC CONFIGURATIONS: 
  # https://kafka.apache.org/documentation/#topicconfigs
  # NOTE: retention.ms=172800000 means after 2d the messages are deleted from the topics.
  topics:
   - name: utenti
     partitions: 1
     replicationFactor: 1
     config:
      retention.ms: "172800000"
   - name: comportamenti
     partitions: 1
     replicationFactor: 1
     config:
      retention.ms: "172800000"
   - name: premi
     partitions: 1
     replicationFactor: 1
     config:
      retention.ms: "172800000"


##
## Zookeeper chart configuration
##
## https://github.com/bitnami/charts/blob/master/bitnami/zookeeper/values.yaml
##
zookeeper:
  enabled: true
  auth:
    ## Enable Zookeeper auth
    ##
    enabled: false
    ## User that will use Zookeeper clients to auth
    ##
    # clientUser:
    ## Password that will use Zookeeper clients to auth
    ##
    # clientPassword:
    ## Comma, semicolon or whitespace separated list of user to be created. Specify them as a string, for example: "user1,user2,admin"
    ##
    # serverUsers:
    ## Comma, semicolon or whitespace separated list of passwords to assign to users when created. Specify them as a string, for example: "pass4user1, pass4user2, pass4admin"
    ##
    # serverPasswords:

## This value is only used when zookeeper.enabled is set to false
##
externalZookeeper:
  ## Server or list of external zookeeper servers to use.
  ##
  servers: []

## Extra init containers to add to the deployment
##
initContainers: []




## EXTRA OBJECTS TO DEPLOY (VALUE EVALUATED AS A TEMPLATE) : KAKFA HDFS SINK CONNECTOR
## It is a Yaml list of multi-line string. So use the - as block sequence entry indicator for each K8s resources you want to deploy,
## followed by | to say that the element of the list is a multi-line string.
##

extraDeploy:
  - |
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: {{ include "kafka.fullname" . }}-connect
      labels: {{- include "common.labels.standard" . | nindent 4 }}
        app.kubernetes.io/component: connector
    spec:
      replicas: 1
      selector:
        matchLabels: {{- include "common.labels.matchLabels" . | nindent 6 }}
          app.kubernetes.io/component: connector
      template:
        metadata:
          labels: {{- include "common.labels.standard" . | nindent 8 }}
            app.kubernetes.io/component: connector
        spec:
          # containers:
          #   - name: install
          #     image: python:3
          #     command: ["/bin/sh", "-c"]
          #     args: ["pip install -t /additionalSoftware kafka-connect-healthcheck; sleep infinity;"]
          #     volumeMounts:
          #     - name: extra-installation
          #       mountPath: "/additionalSoftware"
          # An initial container to install some useful packages:
          initContainers:
          - name: installer
            image: python:3
            command: ["/bin/sh", "-c"]
            args: ["pip install -t /additionalSoftware kafka-connect-healthcheck"]
            volumeMounts:
            - name: extra-installation
              mountPath: "/additionalSoftware"
          containers:
            - name: connect
              image: confluentinc/cp-kafka-connect-base:6.1.0
              imagePullPolicy: IfNotPresent
              command: ["/bin/sh", "-c"]
              args:
                - confluent-hub install --no-prompt confluentinc/kafka-connect-hdfs3:1.1.1 && /opt/bitnami/kafka/scripts/launch-hdfs-sink-connector.sh;
                  export PYTHONPATH=$PYTHONPATH:/additionalSoftware/ && python /additionalSoftware/bin/kafka-connect-healthcheck;
              ports:
                - name: connector
                  containerPort: 8083
              volumeMounts:
                - name: configuration
                  mountPath: /opt/bitnami/kafka/config
                - name: create-connector-script
                  mountPath: /opt/bitnami/kafka/scripts
                - name: extra-installation
                  mountPath: "/additionalSoftware"
              livenessProbe:
                httpGet:
                  path: /
                  port: 18083
                failureThreshold: 1
                periodSeconds: 20
              startupProbe:
                httpGet:
                  path: /
                  port: 18083
                failureThreshold: 15
                periodSeconds: 20
          volumes:
            - name: configuration
              configMap:
                name: {{ include "kafka.fullname" . }}-connect-config
            - name: create-connector-script
              configMap:
                name: {{ include "kafka.fullname" . }}-connect-script
                defaultMode: 0777
            - name: extra-installation
              emptyDir: {}
  - |
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: {{ include "kafka.fullname" . }}-connect-config
      labels: {{- include "common.labels.standard" . | nindent 4 }}
        app.kubernetes.io/component: connector
    data:
      worker.properties: |-
        # Bootstrap Kafka servers. If multiple servers are specified, they should be comma-separated.
        bootstrap.servers = {{ include "kafka.fullname" . }}-0.{{ include "kafka.fullname" . }}-headless.{{ .Release.Namespace }}.svc.{{ .Values.clusterDomain }}:{{ .Values.service.port }}
        
        # The group ID is a unique identifier for the set of workers that form a single Kafka Connect cluster
        group.id = connect-cluster

        plugin.path = /usr/share/confluent-hub-components/confluentinc-kafka-connect-hdfs3/
        key.converter = org.apache.kafka.connect.storage.StringConverter
        value.converter = org.apache.kafka.connect.json.JsonConverter
        key.converter.schemas.enable=false
        value.converter.schemas.enable=false

        # The topic where connector and task configuration data are stored.
        config.storage.topic = connect-configs
        config.storage.replication.factor = 1
        
        # The topic where connector and task configuration offsets are stored.
        offset.storage.topic = connect-offsets
        offset.storage.replication.factor = 1

        # The topic where connector and task configuration status updates are stored .
        status.storage.topic = connect-status
        status.storage.replication.factor = 1
        # confluent.topic.replication.factor = 1

        # Confluent Control Center Integration -- uncomment these lines to enable Kafka client interceptors
        # that will report audit data that can be displayed and analyzed in Confluent Control Center
        # producer.interceptor.classes=io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor
        # consumer.interceptor.classes=io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor

        # The config storage topic must have a single partition, and this cannot be changed via properties. 
        # Offsets for all connectors and tasks are written quite frequently and therefore the offset topic
        # should be highly partitioned; by default it is created with 25 partitions, but adjust accordingly
        # with the number of connector tasks deployed to a distributed worker cluster. Kafka Connect records
        # the status less frequently, and so by default the topic is created with 5 partitions.
        #offset.storage.partitions=25
        #status.storage.partitions=5

  - |
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: {{ include "kafka.fullname" . }}-connect-script
      labels: {{- include "common.labels.standard" . | nindent 4 }}
        app.kubernetes.io/component: connector
    data:
      launch-hdfs-sink-connector.sh: |-
        #!/bin/sh
        # 1. Launch (in background) Kafka Connect Worker in distributed mode
        echo -e "\n\n=============\nLaunching the Kafka Connect WORKER in Distributed mode...\n=============\n"
        /bin/connect-distributed /opt/bitnami/kafka/config/worker.properties &
        
        # 2.Wait for Kafka connect REST listener
        echo -e "\n\n=============\nWaiting for Kafka Connect to start listening on localhost ⏳\n=============\n"
        while [ $(curl -s -o /dev/null -w %{http_code} http://localhost:8083/connectors) -ne 200 ] ; do
          echo -e "\t" $(date) " Kafka Connect listener HTTP state: " $(curl -s -o /dev/null -w %{http_code} http://localhost:8083/connectors) " (waiting for 200)"
          sleep 5
        done
        echo -e $(date) "\n\n--------------\n\o/ Kafka Connect is ready! Listener HTTP state: " $(curl -s -o /dev/null -w %{http_code} http://localhost:8083/connectors) "\n--------------\n"

        # 3. Create Hdfs sink connector with a REST call using curl
        echo -e "\n\n=============\nCreating the specific CONNECTOR with a REST call...\n=============\n"
        curl -X PUT \
          -H "Content-Type: application/json" \
          --data '{
                     "_comment": "--- The HDFS SINK connector class ---",
                    "connector.class"                         : "io.confluent.connect.hdfs3.Hdfs3SinkConnector",
                    
                    "_comment": " --- HDFS SINK-specific configuration below here  --- ",
                    "tasks.max"                               : "3",
                    "topics"                                  : "utenti, comportamenti, premi",
                    "hdfs.url"                                : "hdfs://my-hdfs-namenodes:8020",
                    "hadoop.conf.dir"                         : "/etc/hadoop/",
                    "hadoop.home"                             : "/opt/hadoop-3.1.3/share/hadoop/common",
                    "logs.dir"                                : "/tmp",
                    "confluent.topic.bootstrap.servers"       : "{{ include "kafka.fullname" . }}-0.{{ include "kafka.fullname" . }}-headless.{{ .Release.Namespace }}.svc.{{ .Values.clusterDomain }}:{{ .Values.service.port }}",

                    "_comment": " --- Partition Storage Strategy  --- ",
                    "partitioner.class"                       : "io.confluent.connect.storage.partitioner.TimeBasedPartitioner",
                    "path.format"                             : "YYYY/MM/dd/HH",
                    "partition.duration.ms"                   : "60000",
                    "locale"                                  : "fr-FR",
                    "timezone"                                : "Europe/Paris",
                    "flush.size"                              : "1000000",
                    "rotate.schedule.interval.ms"             : "180000",
                    "format.class"                            : "io.confluent.connect.hdfs3.json.JsonFormat",
                    "topics.dir"                              : "/HeraSDG/raw_data",
                    "filename.offset.zero.pad.width"          : "6",
                    
                    "_comment": " --- Single Message Transformation: Filtering  --- ",                    
                    "transforms"                              : "dropNullRecords",
                    "transforms.dropNullRecords.type"         : "org.apache.kafka.connect.transforms.Filter",
                    "transforms.dropNullRecords.predicate"    : "isNullRecord",
                     
                    "predicates"                              : "isNullRecord",
                    "predicates.isNullRecord.type"            : "org.apache.kafka.connect.transforms.predicates.RecordIsTombstone"
                }' \
          http://localhost:8083/connectors/hdfs3-sink/config

      # 4. Monitoring the connectors
      # while ( $(curl -s -o /dev/null -w %{http_code} http://localhost:8083/connectors) -eq 200 && $(curl http://localhost:8083/connectors/hdfs3-sink/status) | python -c "import sys, json; print json.load(sys.stdin)['id']")
  - | 
    apiVersion: v1
    kind: Service
    metadata:
      name: {{ include "kafka.fullname" . }}-connect
      labels: {{- include "common.labels.standard" . | nindent 4 }}
        app.kubernetes.io/component: connector
    spec:
      ports:
        - protocol: TCP
          port: 8083
          targetPort: connector
      selector: {{- include "common.labels.matchLabels" . | nindent 4 }}
        app.kubernetes.io/component: connector